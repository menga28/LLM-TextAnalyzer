{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ollama) (2.10.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\menga\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\menga\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.join(\"app\",\"llm_model\")\n",
    "MODELS = [\n",
    "    {\n",
    "        \"id\": \"mistral-7b\",\n",
    "        \"filename\": \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "        \"path\": os.path.join(MODEL_DIR, \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"),\n",
    "        \"url\": \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "        \"hash_md5\": \"d98804ecfe3f274b46aed70d9257945e\",\n",
    "        \"prompt_template\": \"\"\"[INST] You are an AI model designed to answer questions based on academic papers. You have access to the title, abstract, and a specific question related to the paper. Your task is to provide an answer that is focused and directly related to the content of the paper. Make sure to:- Base your response only on the title and abstract of the paper.- Answer the question as specifically as possible.- If the question cannot be answered with the given information, politely indicate that the answer is unclear based on the provided content.[/INST]\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"llama-3.2-3b\",\n",
    "        \"filename\": \"Llama-3.2-3B-Instruct-uncensored-Q8_0.gguf\",\n",
    "        \"path\": os.path.join(MODEL_DIR, \"Llama-3.2-3B-Instruct-uncensored-Q8_0.gguf\"),\n",
    "        \"url\": \"https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q8_0.gguf\",\n",
    "        \"hash_md5\": \"08dd494d754c926afddc737400ab6c91\",\n",
    "        \"prompt_template\": \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are an AI model designed to answer questions based on academic papers. You have access to the title, abstract, and a specific question related to the paper. Your task is to provide an answer that is focused and directly related to the content of the paper. Make sure to:- Base your response only on the title and abstract of the paper.- Answer the question as specifically as possible.- If the question cannot be answered with the given information, politely indicate that the answer is unclear based on the provided content.{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"DeepSeek-R1\",\n",
    "        \"filename\": \"DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\",\n",
    "        \"path\": os.path.join(MODEL_DIR, \"DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\"),\n",
    "        \"url\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\",\n",
    "        \"hash_md5\": \"1289bc061e5d0a4e7f3382f9d29af3b0\",\n",
    "        \"prompt_template\": \"\"\"{prompt}\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = MODELS[2][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "httpx.Client() got multiple values for keyword argument 'base_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mllm_model\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:9000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODELS[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m     from_\u001b[38;5;241m=\u001b[39mMODELS[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     system\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\menga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ollama\\_client.py:114\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, host, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, host: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhttpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\menga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ollama\\_client.py:91\u001b[0m, in \u001b[0;36mBaseClient.__init__\u001b[1;34m(self, client, host, follow_redirects, timeout, headers, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     76\u001b[0m   client,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m  Creates a httpx client. Default parameters are the same as those defined in httpx\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  except for the following:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m  `kwargs` are passed to the httpx client.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client(\n\u001b[0;32m     92\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m_parse_host(host \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOLLAMA_HOST\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     93\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m     94\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Lowercase all headers to ensure override\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     97\u001b[0m       k\u001b[38;5;241m.\u001b[39mlower(): v\n\u001b[0;32m     98\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama-python/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplatform\u001b[38;5;241m.\u001b[39mmachine()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplatform\u001b[38;5;241m.\u001b[39msystem()\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Python/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplatform\u001b[38;5;241m.\u001b[39mpython_version()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    103\u001b[0m       }\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    104\u001b[0m     },\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    106\u001b[0m   )\n",
      "\u001b[1;31mTypeError\u001b[0m: httpx.Client() got multiple values for keyword argument 'base_url'"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "path = r\"app\\llm_model\\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf\"\n",
    "\n",
    "client = ollama.Client(base_url=\"http://localhost:9000\")\n",
    "\n",
    "response = client.create(\n",
    "    model = MODELS[2][\"id\"],\n",
    "    from_=MODELS[2][\"path\"],\n",
    "    system=\"You are a helpful assistant.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
